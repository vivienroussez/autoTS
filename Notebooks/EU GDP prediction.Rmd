---
title: "Package autoTS : automatic model selection <br> for univariate time series prediction"
output:
  html_notebook:
    toc: true
    toc_float: true
---

```{r}
knitr::opts_chunk$set(warning = F,message = F)
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(lubridate))
library(autoTS)
```


# Introduction

## What does this package do ?

The `autoTS` package provides a high level interface for **univariate time series** predictions. It implements many algorithms, most of them provided by the `forecast` package. The main goals of the package are :

- Simplify the preparation of the time series ;
- Train the algorithms and compare their results, to chose the best one ;
- Gather the results in a final **tidy dataframe** 

## What are the inputs ?

The package is designed to work on one time series at a time. Parallel calculations can be put on top of it (see example below). The user has to provide 2 simple vectors :

- One with the dates (s.t. the `lubridate` package can parse them)
- The second with the corresponding values

## Warnings

This package implements each algorithm with a unique parametrization, meaning that the user cannot tweak the algorithms (eg modify SARIMA specfic parameters). 

# Exemple on real-world data

For this example, we will use the GDP quarterly data of the european countries provided by eurostat. The database can be downloaded from [this page](https://ec.europa.eu/eurostat/web/national-accounts/data/database) and then chose "GDP and main components (output, expenditure and income) (namq_10_gdp)" and then adjust the time dimension to select all available data and download as a csv file with the correct formatting (1 234.56). The csv is in the "Data" folder of this notebook.

```{r}
dat <- read.csv("Data/namq_10_gdp_1_Data.csv")
str(dat)
head(dat)
```

## Data preparation

First, we have to clean the data (not too ugly though). First thing is to convert the TIME column into a well known date format that lubridate can handle. In this example, `yq` function can parse the date without modification of the column. Then, we have to remove the blank in the values that separates thousands...
Finally, we only keep data since 2000 and the unadjusted series in current prices.

After that, we should get one time series per country

```{r}
dat <- mutate(dat,dates=yq(as.character(TIME)),
              values = as.numeric(stringr::str_remove(Value," "))) %>% 
  filter(year(dates)>=2000 & S_ADJ=="Unadjusted data (i.e. neither seasonally adjusted nor calendar adjusted data)" &
           UNIT == "Current prices, million euro")

filter(dat,GEO %in% c("France","Austria")) %>% 
  ggplot(aes(dates,values,color=GEO)) + geom_line() + theme_light() +
  labs(title="GDP of (completely) random countries")
```

Now we're good to go !

## Prediction on a random country

Let's see how to use the package on one time series :

- Extract dates and values of the time series you want to work on
- Create the object containing all you need afterwards
- Train algo and determine which one is the best (over the last known year)
- Implement the best algorithm on full data

```{r}
ex1 <- filter(dat,GEO=="France") 
preparedTS <- prepare.ts(ex1$dates,ex1$values,"quarter")

## What is in this new object ?
str(preparedTS)
plot.ts(preparedTS$obj.ts)
ggplot(preparedTS$obj.df,aes(dates,val)) + geom_line() + theme_light()

## What is the best model for prediction ?
best.algo <- getBestModel(ex1$dates,ex1$values,"quarter",bagged = T,graph = F)
names(best.algo)
print(paste("The best algorithm is",best.algo$best))
plotly::ggplotly(best.algo$graph.train)

## Build the predictions
final.pred <- my.predictions(preparedTS,best.algo$best)
tail(final.pred,24)
ggplot(final.pred) + geom_line(aes(dates,actual.value),color="black") + 
  geom_line(aes(dates,prophet,linetype=type),color="red") +
  theme_light() 
```

Not too bad, right ?

# Scaling predictions

Let's say we want to make a prediction for each country in the same time and be the fastest possible $\rightarrow$ let's combine the package's functions with parallel computing. We have to reshape the data to get one column per country and then iterate over the columns of the data frame.

## Prepare data 

```{r}
suppressPackageStartupMessages(library(tidyr))
dat.wide <- select(dat,GEO,dates,values) %>% 
  group_by(dates) %>% 
  spread(key = "GEO",value = "values")
head(dat.wide)
```

## Compute bulk predictions

```{r}
library(doParallel)
pipeline <- function(dates,values)
{
  bm <- getBestModel(dates,values,"quarter",graph = F)
  pred <- prepare.ts(dates,values,"quarter") %>% 
    my.predictions(bm$best)
  return(pred)
}
doMC::registerDoMC(parallel::detectCores()-1) # parallel backend (for UNIX)

system.time({
  res <- foreach(ii=2:ncol(dat.wide),.packages = c("dplyr","autoTS")) %dopar%
  pipeline(dat.wide$dates,pull(dat.wide,ii))
})
names(res) <- colnames(dat.wide)[-1]
str(res)
```

## There is no free lunch...

There is no best algorithm in general $\Rightarrow$ depends on the data !

```{r}
sapply(res,function(xx) colnames(select(xx,-dates,-type,-actual.value)) ) %>% table()
sapply(res,function(xx) colnames(select(xx,-dates,-type,-actual.value)) )
```

